{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MU7k0WUN7dPF"
      },
      "outputs": [],
      "source": [
        "#what\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USrS6My477ym"
      },
      "outputs": [],
      "source": [
        "xdf = pd.read_csv('/content/drive/MyDrive/ML/Datasets/Chinese_segmentation/as_training.utf8', header=None, names=['raw_words'])\n",
        "xfor_vocab = open('/content/drive/MyDrive/ML/Datasets/Chinese_segmentation/as_training.utf8')\n",
        "\n",
        "#testdf = pd.read_csv('/content/drive/MyDrive/ML/Datasets/Chinese_segmentation/as_test.utf8', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "xdf = pd.read_csv(r'C:\\Users\\zhiha\\OneDrive\\Documents\\.Zhihao\\.ML\\datasets\\Chinese Segmentation\\as_training.utf8', header=None, names=['raw_words'])\n",
        "#xfor_vocab = open('/content/drive/MyDrive/ML/Datasets/Chinese_segmentation/as_training.utf8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Mie5F4TgCjGk"
      },
      "outputs": [],
      "source": [
        "def spaceornot(line):\n",
        "  splace = []\n",
        "  listed = list(line)\n",
        "  for i, char in enumerate(listed):\n",
        "    if listed[i] == listed[-1]:\n",
        "      splace.append(1)\n",
        "    elif char == '\\u3000':\n",
        "      pass\n",
        "    elif listed[i + 1] == '\\u3000':\n",
        "      splace.append(1)\n",
        "    elif listed[i + 1] != '\\u3000':\n",
        "      splace.append(0)\n",
        "  return splace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "M1RHsgw8-heC",
        "outputId": "7fcbe017-aa2f-4acc-846c-43d025435ee5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nused data:\\ntrain_data: xseq\\ntrain_label: yactual\\n'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yraw = xdf.copy()\n",
        "\n",
        "xtrain = pd.DataFrame(xdf['raw_words'].str.replace('\\u3000',''))   #could ignore last punctuation\n",
        "\n",
        "#tokening _input data\n",
        "tokenizer = Tokenizer(char_level=True, oov_token='<OOV>')\n",
        "        #dlist_sent = seriestolist(xtrain, 0)\n",
        "tokenizer.fit_on_texts(xtrain['raw_words'])\n",
        "word_index = tokenizer.word_index\n",
        "xtrain['sequenced_sent'] = tokenizer.texts_to_sequences(xtrain['raw_words'])\n",
        "X = pad_sequences(xtrain['sequenced_sent'], padding='post')\n",
        "\n",
        "#y_actual\n",
        "yraw['labels'] = yraw['raw_words'].map(spaceornot)\n",
        "Y = pad_sequences(yraw['labels'], padding='post')\n",
        "\n",
        "\n",
        "# alternatively, you can pass the padded arrays into the dataframe by converting it into a list using list()\n",
        "\"\"\"\n",
        "used data:\n",
        "train_data: xseq\n",
        "train_label: yactual\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VNm4SbcZ3Jgu"
      },
      "outputs": [],
      "source": [
        "val_split = 0.75\n",
        "vsize = len(word_index)\n",
        "sentlen = len(X[0,:])\n",
        "em_dim = 512\n",
        "epol = 1000\n",
        "basize = 512\n",
        "\n",
        "#callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aky4VCs3Ks0M"
      },
      "outputs": [],
      "source": [
        "#splitting dataset\n",
        "spli = int(val_split * len(X))\n",
        "trainX, testX = X[:spli], X[spli:]\n",
        "trainY, testY = Y[:spli], Y[spli:]\n",
        "\n",
        "\n",
        "#possible to split in ~model~.fit() function too"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ngt7nEm7h22X"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Custom Loss\n",
        "'''\n",
        "\n",
        "class MaskedSequenceLoss(tf.keras.losses.Loss):\n",
        "    def __init__(\n",
        "        self,\n",
        "        average_across_timesteps=False,\n",
        "        average_across_batch=False,\n",
        "        sum_over_timesteps=True,\n",
        "        sum_over_batch=True,\n",
        "        softmax_loss_function=None,\n",
        "        name=None,\n",
        "        reduction=None, # dummy arg so it can be used as custom object when loading saved model\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.opts = {\n",
        "            \"average_across_timesteps\": average_across_timesteps,\n",
        "            \"average_across_batch\": average_across_batch,\n",
        "            \"sum_over_timesteps\": sum_over_timesteps,\n",
        "            \"sum_over_batch\": sum_over_batch,\n",
        "            \"softmax_loss_function\": softmax_loss_function,\n",
        "            \"name\": name,\n",
        "        }\n",
        "    \n",
        "    def call(self, y_true, y_pred):\n",
        "        return tfa.seq2seq.sequence_loss(y_pred, y_true,\n",
        "                                         weights=tf.cast(y_pred._keras_mask, tf.float32) if hasattr(y_pred, \"_keras_mask\") else tf.ones(y_true.shape),\n",
        "                                         **self.opts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "em_dim_eff = 128\n",
        "\n",
        "\n",
        "xin = Input((sentlen,))\n",
        "\n",
        "x = Embedding(vsize, em_dim_eff, input_length=sentlen, mask_zero=True)(xin)\n",
        "x = Bidirectional(LSTM(64, return_sequences = True))(x)\n",
        "\n",
        "x = Dense(64, activation='swish')(x)\n",
        "x =Dropout(0.5)(x)\n",
        "\n",
        "xout = Dense(2, activation='linear')(x)\n",
        "\n",
        "effsegcl = Model(xin, xout)\n",
        "effsegcl.compile(optimizer='adam', loss=MaskedSequenceLoss(), metrics=['accuracy'])\n",
        "effsegcl.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL1QUTngMliB",
        "outputId": "f749ecc3-e0c6-463b-a26f-ecb81e869214"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_19 (InputLayer)       [(None, 188)]             0         \n",
            "                                                                 \n",
            " embedding_7 (Embedding)     (None, 188, 512)          3115520   \n",
            "                                                                 \n",
            " bidirectional_15 (Bidirecti  (None, 188, 128)         295424    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_16 (Bidirecti  (None, 188, 128)         98816     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_75 (Dense)            (None, 188, 256)          33024     \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 188, 256)          0         \n",
            "                                                                 \n",
            " dense_76 (Dense)            (None, 188, 64)           16448     \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 188, 64)           0         \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 188, 2)            130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,559,362\n",
            "Trainable params: 3,559,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "xin = Input((sentlen,))\n",
        "\n",
        "x = Embedding(vsize, em_dim, input_length=sentlen, mask_zero = True)(xin) #ignore padded zeros -> mask_zero = True\n",
        "x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
        "x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
        "\n",
        "x = Dense(256, activation='swish')(x)\n",
        "x = Dropout(0.7)(x)\n",
        "x = Dense(64, activation='swish')(x)\n",
        "x = Dropout(0.7)(x)\n",
        "\n",
        "xout = Dense(2, activation='linear')(x)\n",
        "\n",
        "segcl = Model(xin, xout)\n",
        "segcl.compile(optimizer='adam', loss=MaskedSequenceLoss(), metrics=['accuracy'])\n",
        "segcl.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xin = Input((sentlen, 1))\n",
        "x = tf.expand_dims(xin, axis=1)\n",
        "x = Flatten()(x)\n",
        "\n",
        "x = Dense(1024, activation='swish')(xin)\n",
        "x = Dense(512, activation='swish')(x)\n",
        "x = Dense(256, activation='swish')(x)\n",
        "x = Dense(256, activation='swish')(x)\n",
        "\n",
        "xout1 = Dense(sentlen)(x)\n",
        "xout2 = Dense(sentlen)(x)\n",
        "\n",
        "xout = tf.stack([xout1, xout2], axis=2)\n",
        "\n",
        "FULL = Model(xin, xout)\n",
        "FULL.compile(optimizer='adam', loss=MaskedSequenceLoss(), metrics=['accuracy'])\n",
        "FULL.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "dibiJmHx3rX3"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, factor=0.1, verbose=1)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "yQAuG_pJP1Hg",
        "outputId": "78e32627-bb52-4055-f5b2-0f3e516f4be1"
      },
      "outputs": [],
      "source": [
        "FULL.fit(\n",
        "    trainX, \n",
        "    trainY,\n",
        "    validation_data=(testX, testY),\n",
        "    epochs = epol,\n",
        "    batch_size = basize,\n",
        "    callbacks = callbacks\n",
        ")\n",
        "\n",
        "#you can use validation_split = True to split the data into val and train, instead of manual splitting\n",
        "#using tf.stack, you can pass the dataframe series into it directly\n",
        "'''\n",
        "segcl.fit(\n",
        "\n",
        ")\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def splitter(sentences, text_split_indexes):\n",
        "    dat = pd.DataFrame(sentences, columns=['unsplit'])\n",
        "    splited = []\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        split_sen = ''\n",
        "        for n, sply in enumerate(list(text_split_indexes[i])[:len(sentence)]):\n",
        "            split_sen += sentence[n]\n",
        "            if sply == 1:\n",
        "                split_sen += '\\u3000'\n",
        "\n",
        "        splited.append(split_sen)\n",
        "    dat['splited'] = pd.DataFrame(splited)\n",
        "\n",
        "    return dat\n",
        "\n",
        "def display(dataframe, start=0, stop=None):\n",
        "    for i in range(len(dataframe.iloc[start:stop,0])):\n",
        "        print('original sentence:  ' + dataframe.iloc[i, 0])\n",
        "        print('splitted sentence:  ' + dataframe.iloc[i, 1])\n",
        "        print()\n",
        "\n",
        "\n",
        "def display_with_correct(dataframe, correct, start=0, stop=None):\n",
        "    for i in range(len(dataframe.iloc[start:stop,0])):\n",
        "        print('original sentence:  ' + dataframe.iloc[i, 0])\n",
        "        print('splitted sentence:  ' + dataframe.iloc[i, 1])\n",
        "        print('actually sentence:  ' + correct.iloc[i, 0])\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_sentences=[\n",
        "    '生日快樂',\n",
        "    '我的名字是',\n",
        "    '今天的天氣很不可思議',\n",
        "    '台北市國稅局南港稽征處定三月十一日上午九時卅分起至下午四時在活動中心一樓設服務台協助同仁處理綜合所得稅結算申報事宜'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_sentences = list(xtrain.iloc[:10,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WzVTGrYPYxOd"
      },
      "outputs": [],
      "source": [
        "token_sentences = tokenizer.texts_to_sequences(input_sentences)\n",
        "token_sentences = pad_sequences(token_sentences, padding='post', maxlen=sentlen)\n",
        "\n",
        "text_predictions = segcl.predict(token_sentences)\n",
        "text_predictions = tf.nn.softmax(text_predictions)\n",
        "text_predictions = np.argmax(text_predictions, axis=-1)\n",
        "\n",
        "org_data = splitter(input_sentences, text_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display_with_correct(org_data, xdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original sentence:  生日快樂\n",
            "splitted sentence:  生日　快樂　\n",
            "\n",
            "original sentence:  我的名字是\n",
            "splitted sentence:  我　的　名字　是　\n",
            "\n",
            "original sentence:  今天的天氣很不可思議\n",
            "splitted sentence:  今天　的　天氣　很　不可思議　\n",
            "\n",
            "original sentence:  台北市國稅局南港稽征處定三月十一日上午九時卅分起至下午四時在活動中心一樓設服務台協助同仁處理綜合所得稅結算申報事宜\n",
            "splitted sentence:  台北市　國稅局　南港　稽征處　定　三月　十一日　上午　九時　卅分　起　至　下午　四時　在　活動　中心　一樓　設　服務台　協助　同仁　處理　綜合　所　得稅　結算　申報　事宜　\n",
            "\n"
          ]
        }
      ],
      "source": [
        "display(org_data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Chinese_segmentation.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "38c7ce74dd526bc9e84fd0682f6c1ac8fcd6c4cb0e87d36fcf4e0e214217cc07"
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
